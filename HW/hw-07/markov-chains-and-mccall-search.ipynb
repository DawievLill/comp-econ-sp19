{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains and Dynamic Programming\n",
    "\n",
    "In this lecture, we'll practice constructing and simulating finite state Markov chains and solving dynamic economic models using dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "from quantecon.distributions import BetaBinomial\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 1\n",
    "\n",
    "**Using `quantecon`, create a MarkovChain object that has the following properties. Simulate it for `T=100`. Plot the simulation.**\n",
    "\n",
    "*Description of Markov Process*\n",
    "\n",
    "Wages can be in one of three states: 10, 20, or 30.\n",
    "If $w_t = 10$, then there is a .5 probability that $w_{t+1}=10$ and a .5 probability that $w_{t+1} = 20$. If $w_t = 20$, there is a .5 probability that $w_{t+1} = 20$, a .25 probability that $w_{t+1} = 10$ and .25 probability that $w_{t+1} = 30$. If $w_t = 30$, then there is a .5 probability that $w_{t+1} = 30$ and a .5 probability that $w_{t+1}= 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 2: Solve for the stationary distribution of the previous Markov chain\n",
    "\n",
    "Solve for the stationary distribution in 3 different ways. Verify that your answers are approximately the same in each case.\n",
    "\n",
    "  - Simulate the Markov chain for many periods ( `T = 100_000` should be close enough) and then compute average visits to each state.\n",
    "  - Compute the matrix power of the stochastic matrix for some large integer power: $P^T$ for some large $T$. `T=10` should be large enough in this case, but you can experiment with larger values.\n",
    "  - Compute the normalized left-eigenvector associated principal left-eigenvector. (Compute the eigenvectors of the transpose of the stochastic matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Construct the following finite state markov chain...\n",
    "\n",
    "Construct the following finite state markov chain. Let the state space of $w_t$ be the natural numbers from 10 to 40. \n",
    "That is, let $w_t \\in \\{10,11,...,39,40\\}$. Construct a Markov chain that has an equal probability of increasing or decreasing and has a 50% chance of staying the same. If the state is at the lower or upper boundary, let the probability of reflecting be 50%. To be more precise, construct a Markov chain with the following properties:\n",
    "\n",
    "  * If $w_t = n$ for $n=11,...39$, let the probability that $w_t = n + 1$ be 25% and let the probability that $w_t = n-1$ be 25%. Let the probability that $w_t = w_{t+1}$ be 50% for all $n=10,...40$. If $w_{t} = 10$, let the probability that $w_{t+1} = 11$ be 50%. If $w_t = 40$, let the probability that $w_{t+1} = 39$ be 50%.\n",
    "  * Try not to construct the matrix by setting each individual entry. You might try to be more clever about it by using a \"for\"-loop or, even better, some kind of vectorized operation.\n",
    "  \n",
    "Use `quantecon` to create a MarkovChain object. Set `np.random.seed(123)` before the simulation and simulate 100 time periods. Plot the simulation.\n",
    "\n",
    "**Hint:** Use `np.linspace` to create the state values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.4\n",
    "\n",
    "**Approximate an AR(1) with a Finite State Markov Chain**\n",
    "\n",
    "This exercise demonstrates a method for approximating *continuous state-space* Markov chains with *finite state-space* Markov chains. Finite state Markov chains are useful because they make computations in some cases easier. \n",
    "\n",
    "Consider the autoregressive stochastic process\n",
    "$$\n",
    "w_{t+1} = (1-\\rho) \\mu + \\rho w_t + \\epsilon_{t+1},\n",
    "$$\n",
    "where $\\epsilon_{t+1}$ is an iid sequence of shocks that are Normally distributed with mean 0 and standard deviation $\\sigma$.\n",
    "Note that by assuming this form, we have the unconditional expectation $E[w_t] = \\mu$ and unconditional variance $Var(w_t) = \\frac{\\sigma^2}{1-\\rho^2}$.\n",
    "\n",
    "Now, as we discussed in the lecture on [Finite State Markov chains](https://lectures.quantecon.org/py/finite_markov.html), we can [approximate this continuous state space markov process with a finite state markov chain.](https://lectures.quantecon.org/py/finite_markov.html#exercise-3)\n",
    "\n",
    "Do the following:\n",
    "\n",
    "  - Approximate this process with `n=50` points, with $\\mu=30$, $\\rho=.9$, and $\\sigma=10$. Write code that uses `quantecon` to build this approximation. Hint: Use the method described here: https://quanteconpy.readthedocs.io/en/latest/markov/approximation.html#quantecon.markov.approximation.rouwenhorst\n",
    "  - Print out the `state_values` used in this approximation. Describe what the `state_values` are.\n",
    "  - Print out the transition probabilities matrix, P. (You don't need to print the whole thing to screen. Just print out enough of it so that we can get an idea of the pattern in it.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCall Search Model Extended\n",
    "\n",
    "In this section, we will solve a variation of the McCall search model. Here we will modify the first model (no possibility of losing a job, etc.). Recall that in the [McCall search model from class](https://lectures.quantecon.org/py/mccall_model.html), we assumed that the wage sequence $\\{w_t\\}$ was iid with a probability mass funtion $p_1,..., p_n$. Here, we will change this assumption and assume that wages follow a finite-state markov process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Modify the McCall Search model and solve...\n",
    "\n",
    "Modify the McCall Search model so that wages were instead governed by the finite state markov chain that we constructed above in question 2. Solve for the value function that is defined like so:\n",
    "\n",
    "$$\n",
    "V(w_t) = \\max \\left\\{ \\frac{w_t}{1-\\beta}, c + \\beta E[V(w_{t+1})\\; | \\; w_t] \\right\\}\n",
    "$$\n",
    "\n",
    "Note that this implies that the conditional expectation depends is conditional (depends on) the current state. The conditional expectation can be computed as a finite sum where the probabilities are read off of the Markov transition matrix.\n",
    "\n",
    "Compute and plot the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
