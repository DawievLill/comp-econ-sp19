{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Demonstration\n",
    "\n",
    "In this lecture, we will demonstrate Monte Carlo methods. This is a useful tool in testing our estimation techniques. Recall that omitting relevant variables in OLS can sometimes lead to biased regression estimates. To see this in action, we will simulate data and run these regressions.  \n",
    "\n",
    "## Theoretical Motivation\n",
    "\n",
    "Recall the example of calculating Pi by throwing darts at a board.\n",
    "\n",
    "**Discussion:** Describe the procedure that would be used to calculate Pi. Why does this work?\n",
    "\n",
    "<img src=\"Pi_30K.gif\"/>\n",
    "\n",
    "**Discussion:** \n",
    "\n",
    "  * Let each dart throw within the circle be a 1. Otherwise, let it be a zero. What is the expected value of this random variable? How would you write this as an integral?\n",
    "  * How can we extend this principle to arbitrary integrals?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "**Monte Carlo Integration**\n",
    "\n",
    "$\\newcommand{\\dd}{\\, \\mathrm{d}}$\n",
    "Suppose we have a function $f$ and we want to evaluate the definite integral $\\int_\\Omega f(x) \\dd x$. Monte Carlo integration works on the principle that we can rewrite an integral as the expected value of some random variable. We can then simulate random numbers from a distribution and use the law of large numbers to argue that the sample mean will converge to the population mean. For example, suppose we have some random variable $X$ that has support $\\Omega$ (that is $X \\in \\Omega$). Let the probability density function (PDF) of this random variable be $\\pi$, so that $\\int_\\Omega \\pi(x) \\dd x = 1$. Define a function $g$ such that $g(x) = f(x)/\\pi(x)$. Then\n",
    "\n",
    "$$\n",
    "E[g(X)] = E\\left[ \\frac{f(X)}{\\pi(X)}\\right] = \\int_\\Omega \\frac{f(x)}{\\pi(x)} \\pi(x) \\dd x = \\int_\\Omega f(x) \\dd x.\n",
    "$$\n",
    "\n",
    "Now, if we can simulate draws of the random variable $X$, the law of large numbers tells us that\n",
    "\n",
    "$$\n",
    "\\text{plim}_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{i=1}^N g(X_i) = E[g(X)] = \\int_\\Omega f(x) \\dd x.\n",
    "$$\n",
    "\n",
    "The choice of distribution for $X$ gives us some flexibility in approximating this integral. Some distributions may have better performance than others (in terms of accuracy with respect to sample size). In the most basic case, we will choose a uniform distribution, so that $\\pi(x) = c$. In this case,\n",
    "\n",
    "$$\n",
    "1 = \\int_\\Omega \\pi(x) \\dd x = \\int_\\Omega c \\dd x = c V,\n",
    "$$\n",
    "\n",
    "where $V$ is the volume of the space $\\Omega$\n",
    "$$\n",
    "V = \\int_\\Omega \\, \\mathrm d x .\n",
    "$$ This implies that our integral can be approximated\n",
    "$$\n",
    "\\text{plim}_{N\\rightarrow \\infty} V \\frac{1}{N} \\sum_{i=1}^N f(X_i) = \\int_\\Omega f(x) \\dd x,\n",
    "$$\n",
    "where $X$ is uniformly distributed over $\\Omega$.\n",
    "This is the formula that you will find on the related Wikipedia article. This topic can be explored further [there.](https://en.wikipedia.org/wiki/Monte_Carlo_integration) Monte Carlo simulation, in general, is a tool to explore properties of distributions that are otherwise hard to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install linearmodels\n",
    "# import linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's integrate a half circle using Monte Carlo integration.\n",
    "$$\n",
    "y^2 + x^2 = 1\n",
    "$$\n",
    "so\n",
    "$$\n",
    "y = \\sqrt[+]{1 - x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x = np.random.uniform(low=-1.0, high=1.0, size=N)\n",
    "y = np.sqrt(1 - x**2) \n",
    "V = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y) * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "x = np.random.uniform(low=-1.0, high=1.0, size=N)\n",
    "y = np.sqrt(1 - x**2) \n",
    "V = 2\n",
    "print('Approximation = ', np.mean(y) * V)\n",
    "print('Built-in value = ', np.pi / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example and Simulation\n",
    "\n",
    "Consider a common source of endogeneity in simple linear models: ommited variables. Suppose the data generating process is given by\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\n",
    "$$\n",
    "where $\\epsilon$ is independent of $x_1$ and $x_2$.\n",
    "\n",
    "If we estimate the equation $y = \\tilde \\beta_0 + \\tilde \\beta_1 x_1 + \\nu$, where $\\nu = \\beta_2 x_2 + \\epsilon$, then it may occur that our estimates of $\\tilde \\beta_1$ will be biased?\n",
    "\n",
    "**Discussion: When will our estimate $\\tilde \\beta_1$ be biased? When will it not be biased?**\n",
    "\n",
    "We can run a quick experiment to find out. Let $x_1$ and $x_2$ be positively correlated and let $\\beta_2$ be equal to 3. Will $\\tilde \\beta_1$ over- or under-estimate $\\beta_1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_ex1(N=50, seed=65594):\n",
    "    if seed == None:\n",
    "        pass\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "    # Let's ensure that x_1 and x_2 are correlated\n",
    "    cov = np.array([[2,1],\n",
    "                    [1,2]])\n",
    "    mean = np.array([0, 0])\n",
    "    X = scipy.stats.multivariate_normal.rvs(mean=mean, cov=cov, size=N)\n",
    "    epsilon = np.random.randn(N,1)\n",
    "    beta0, beta1, beta2 = 0, 1, 1\n",
    "    df = pd.DataFrame(X, columns=['x1', 'x2'])\n",
    "    df['epsilon'] = epsilon\n",
    "    df['y'] = beta0 + beta1 * df.x1 + beta2 * df.x2 + df.epsilon\n",
    "    df = df[['y', 'x1', 'x2', 'epsilon']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simulate_data_ex1()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using classic API (need to create matrices ourselves, including adding the constant)\n",
    "endog = df.y\n",
    "exog = sm.add_constant(df.x1)\n",
    "reg = sm.OLS(endog=endog, exog=exog).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use formula API\n",
    "reg = smf.ols('y ~ x1', df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion: Notice that in this result we underestimated $\\beta_1$. Is this what you expected? Why?**\n",
    "\n",
    "Let's try it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simulate_data_ex1(N=50, seed=10954)\n",
    "reg = smf.ols('y ~ x1', df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df.x1, y=df.epsilon, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df.x1, y=df.x2, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more try. **Do you notice anything strange?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simulate_data_ex1(N=50, seed=87548)\n",
    "reg = smf.ols('y ~ x1', df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simulate_data_ex1(N=50, seed=75100)\n",
    "reg = smf.ols('y ~ x1', df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = simulate_data_ex1(N=50, seed=100)\n",
    "reg = smf.ols('y ~ x1', df).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are occuring simply by chance. In fact, I simulated data many times, looping through various \"seeds\", until I found the results that I wanted. See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta0, beta1, beta2 = 0, 1, 1\n",
    "# M = 1000\n",
    "# biases = np.zeros(M)\n",
    "# for i in range(M):\n",
    "#     df = simulate_data_ex1(N=50, seed=i)\n",
    "#     reg = smf.ols('y ~ x1', df).fit()\n",
    "#     biases[i] = reg.params.x1 - beta1\n",
    "\n",
    "# # # Run the above code with M = 100_000, N=50, betas = 0,1,1\n",
    "# # # np.savetxt('biases_N50_beta011_M100000.txt', biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_min = np.argmin(biases)\n",
    "# biases[ind_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_max = np.argmax(biases)\n",
    "# biases[ind_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_amin = np.argmin(np.abs(biases))\n",
    "# biases[ind_amin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_amax = np.argmax(np.abs(biases))\n",
    "# biases[ind_amax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ind_min, ind_max, ind_amin, ind_amax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted Variable Bias\n",
    "\n",
    "Recall that in the (probability) limit, \n",
    "\n",
    "$$\n",
    "\\tilde \\beta_1^N \\overset{p}{\\rightarrow} \\frac{\\text{Cov}(y, x_1)}{\\text{Var}(x_1)} = \\frac{\\text{Cov}(\\beta_1 x_1 + \\beta_2 x_2, x_1)}{\\text{Var}(x_1)} = \\beta_1 + \\beta_2  \\frac{\\text{Cov}(x_2, x_1)}{\\text{Var}(x_1)}\n",
    "$$\n",
    "\n",
    "We can run a simulation to verify this formula.\n",
    "\n",
    "\n",
    "## Monte Carlo Method\n",
    "\n",
    "We will do `M=1000` Monte-Carlo loops. For each loop, we will create a new dataset with `N=50` data points and then we will estimate the parameters. We will record the parameter estimates from each run. We will analyze the properties of these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.params.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "results = pd.DataFrame(index=range(M), columns=reg.params.index)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[0, :] = reg.params\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "M = 1000\n",
    "results = pd.DataFrame(index=range(M), columns=reg.params.index, dtype=np.float)\n",
    "for m in range(M):\n",
    "    # Set `seed` parameter to `None` so that I don't reset\n",
    "    # the seed every time.\n",
    "    df = simulate_data_ex1(seed=None)\n",
    "    reg = smf.ols('y ~ x1', df).fit()\n",
    "    results.loc[m, :] = reg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(results.x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the theory:\n",
    "\n",
    "$$\n",
    "\\tilde \\beta^N_1 \\overset{p}{\\rightarrow}  \\beta_1 + \\beta_2  \\frac{\\text{Cov}(x_2, x_1)}{\\text{Var}(x_1)}\n",
    "$$\n",
    "\n",
    "We can look above to see how we defined the parameters of our simulation and calculate these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0, beta1, beta2 = 0, 1, 1\n",
    "c = 1\n",
    "v = 2\n",
    "tilde_beta1 = beta1 + beta2 * c/v\n",
    "tilde_beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "M = 1000\n",
    "N = 500\n",
    "results2 = pd.DataFrame(index=range(M), columns=reg.params.index, dtype=np.float)\n",
    "for m in range(M):\n",
    "    # Set `seed` parameter to `None` so that I don't reset\n",
    "    # the seed every time.\n",
    "    df = simulate_data_ex1(N=N, seed=None)\n",
    "    reg = smf.ols('y ~ x1', df).fit()\n",
    "    results2.loc[m, :] = reg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(results.x1, label='$N=50$')\n",
    "sns.distplot(results2.x1, label='$N=500$')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "M = 1000\n",
    "N = 5000\n",
    "results3 = pd.DataFrame(index=range(M), columns=reg.params.index, dtype=np.float)\n",
    "for m in range(M):\n",
    "    # Set `seed` parameter to `None` so that I don't reset\n",
    "    # the seed every time.\n",
    "    df = simulate_data_ex1(N=N, seed=None)\n",
    "    reg = smf.ols('y ~ x1', df).fit()\n",
    "    results3.loc[m, :] = reg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(results.x1, label='$N=50$')\n",
    "sns.distplot(results2.x1, label='$N=500$')\n",
    "sns.distplot(results3.x1, label='$N=5000$')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
